{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download iris dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('iris-data.txt', <http.client.HTTPMessage at 0x1a2879f1c30>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n",
    "    \"iris-data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of data allocated to training and testing.\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Temporary list to append lines of text from `iris-data.txt`\n",
    "tmp_list_iris = []\n",
    "\n",
    "# Temporary set to append labels to (set only keeps one of each label).\n",
    "# i.e. \"Iris-versicolor\", \"Iris-verginica\", \"Iris-setosa\"\n",
    "tmp_set_label = set()\n",
    "\n",
    "# Appends features from same line.\n",
    "features = []\n",
    "\n",
    "# Labels from the same line.\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens file and adds all lines that are not spaces to list as string.\n",
    "with open(\"iris-data.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        if not line.isspace():\n",
    "            tmp_list_iris.append(line)\n",
    "\n",
    "    random.shuffle(tmp_list_iris)\n",
    "\n",
    "for line in tmp_list_iris:\n",
    "    # Splits line by commas into a list\n",
    "    # i.e. ['5.7', '3.8', '1.7', '0.3', 'Iris-setosa']\n",
    "    split_line = line.strip().split(',')\n",
    "\n",
    "    # Expected to be 5.\n",
    "    length_line = len(split_line)\n",
    "\n",
    "    for i in range(length_line - 1):\n",
    "        # Converts string feature values into floats.\n",
    "        split_line[i] = float(split_line[i])\n",
    "\n",
    "    # The label is the last element of the `split_line` array.\n",
    "    # i.e. 'Iris-setosa'\n",
    "    label = split_line[length_line - 1]\n",
    "\n",
    "    # Adds label to temporary set for use later.\n",
    "    tmp_set_label.add(label)\n",
    "\n",
    "    # Appends the array of float values to the features array.\n",
    "    features.append(split_line[:length_line - 1])\n",
    "\n",
    "    # Appends the label to the labels array.\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines the minimum and maximum values for the entire features array.\n",
    "# i.e. max_val = 7.9, min_val = 0.0\n",
    "max_val = max([item for i in features for item in i])\n",
    "min_val = min([item for i in features for item in i])\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(len(features[0])):\n",
    "        # Updates features array to scale between 0.0 and 1.0\n",
    "        features[i][j] = (features[i][j] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of all used labels from set (values are all unique).\n",
    "tmp_list_one_hot_encoding = list(tmp_set_label)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    # Turns the string label value into the corresponding index value.\n",
    "    labels[i] = tmp_list_one_hot_encoding.index(labels[i])\n",
    "\n",
    "# Turns the labels array (now a list of indexes) into a numpy array.\n",
    "label_idx = np.array(labels)\n",
    "\n",
    "# Creates a len(labels) x len(tmp_list_one_hot_encoding) matrix of zeros.\n",
    "# i.e. 150 x 3\n",
    "labels = np.zeros((len(labels), len(tmp_list_one_hot_encoding)))\n",
    "\n",
    "# For each list in array it sets the corresponding label_idx value to 1.\n",
    "# i.e. [[0, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1], ...]\n",
    "labels[np.arange(len(labels)), label_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates training and test sets for features and labels.\n",
    "features_train = np.array(features[:int(train_test_ratio * len(features))])\n",
    "features_test = np.array(features[int(train_test_ratio * len(features)):])\n",
    "\n",
    "labels_train = labels[:int(train_test_ratio * len(labels))]\n",
    "labels_test = labels[int(train_test_ratio * len(labels)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets input layers to the number of features (4).\n",
    "n_input_layers = len(features_test[0])\n",
    "\n",
    "n_hidden_layers = 5\n",
    "\n",
    "# Sets the number of output layers to the number of label types (3).\n",
    "n_output_layers = len(tmp_list_one_hot_encoding)\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "activation_f = {\n",
    "    'identity': lambda x: x,\n",
    "    'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
    "    'tanh': lambda x: np.tanh(x),\n",
    "    'relu': lambda x: x * (x > 0),\n",
    "}\n",
    "\n",
    "# Derivatives of the previous lambda functions.\n",
    "activation_f_prime = {\n",
    "    'identity': lambda x: 1,\n",
    "    'sigmoid': lambda x: x * (1.0 - x),\n",
    "    'tanh': lambda x: 1 - x**2,\n",
    "    'relu': lambda x: 1.0 * (x > 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 'tanh'\n",
    "f2 = 'sigmoid'\n",
    "\n",
    "act_f1 = activation_f[f1]\n",
    "act_f2 = activation_f[f2]\n",
    "\n",
    "act_f1_prime = activation_f_prime[f1]\n",
    "act_f2_prime = activation_f_prime[f2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_features, output_label, i_h_weights, h_o_weights):\n",
    "    input_features = input_features.reshape(1, -1)\n",
    "\n",
    "    # forward prop\n",
    "    h_inter = input_features @ i_h_weights\n",
    "    h_result = act_f1(h_inter)\n",
    "    o_inter = h_result @ h_o_weights\n",
    "    o_result = act_f2(o_inter)\n",
    "\n",
    "    error = np.mean(0.5 * np.square(o_result - output_label))\n",
    "\n",
    "    # back prop\n",
    "    del_h_o = -np.multiply(output_label - o_result, act_f2_prime(o_result))\n",
    "    change_h_o = h_result.T @ del_h_o\n",
    "    del_i_h = np.dot(del_h_o, h_o_weights.T) * act_f1_prime(h_result)\n",
    "    change_i_h = input_features.T @ del_i_h\n",
    "\n",
    "    return error, change_i_h, change_h_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses just forward prop\n",
    "def predict(input_features, i_h_weights, h_o_weights):\n",
    "    h_inter = input_features @ i_h_weights\n",
    "    h_result = act_f1(h_inter)\n",
    "    o_inter = h_result @ h_o_weights\n",
    "    o_result = act_f2(o_inter)\n",
    "    return (o_result >= max(o_result)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Train ***********\n",
      "Epoch: 0  Train-error: 0.10989753036080718  Validation-error: 0.08803014139909461\n",
      "Epoch: 10  Train-error: 0.037157827031363104  Validation-error: 0.03274245777332146\n",
      "Epoch: 20  Train-error: 0.02123807603893122  Validation-error: 0.019283277257004537\n",
      "Epoch: 30  Train-error: 0.01666350436779533  Validation-error: 0.015397931897714021\n",
      "Epoch: 40  Train-error: 0.014211889231652569  Validation-error: 0.014806383025108992\n",
      "Epoch: 50  Train-error: 0.01193071677410847  Validation-error: 0.008617168512610938\n",
      "Epoch: 60  Train-error: 0.00948568533990954  Validation-error: 0.009958701905327935\n",
      "Epoch: 70  Train-error: 0.008640393057259436  Validation-error: 0.007859707776487382\n",
      "Epoch: 80  Train-error: 0.008499162764819592  Validation-error: 0.00805293813794774\n",
      "Epoch: 90  Train-error: 0.008237914044169091  Validation-error: 0.008302529059961214\n"
     ]
    }
   ],
   "source": [
    "print(\"*********** Train ***********\")\n",
    "\n",
    "# Initial Random Weights\n",
    "V = np.random.normal(scale=0.1, size=(n_input_layers, n_hidden_layers))\n",
    "W = np.random.normal(scale=0.1, size=(n_hidden_layers, n_output_layers))\n",
    "\n",
    "# Training-set\n",
    "X = features_train\n",
    "T = labels_train\n",
    "\n",
    "# Epoch-training\n",
    "for epoch in range(n_epoch):\n",
    "    tr_err = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        loss, grad_V, grad_W = train(X[i], T[i], V, W)\n",
    "\n",
    "        # Adjust Weights\n",
    "        V -= learning_rate * grad_V + momentum * grad_V\n",
    "        W -= learning_rate * grad_W + momentum * grad_W\n",
    "\n",
    "        tr_err.append(loss)\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        val_err = []\n",
    "        \n",
    "        # use test set as validiation set\n",
    "        for i in range(features_test.shape[0]):\n",
    "            loss, _, _ = train(features_test[i], labels_test[i], V, W)\n",
    "            val_err.append(loss)\n",
    "            \n",
    "        train_error = sum(tr_err) / len(tr_err)\n",
    "        valid_error = sum(val_err) / len(val_err)\n",
    "        \n",
    "        print(\"Epoch:\", epoch, \" Train-error:\", train_error, \" Validation-error:\", valid_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Test ***********\n",
      "Total = 30 Success = 29 Accuracy = 96.666667\n"
     ]
    }
   ],
   "source": [
    "print(\"*********** Test ***********\")\n",
    "\n",
    "success = 0\n",
    "for i in range(len(features_test)):\n",
    "    a = predict(features_test[i], V, W)\n",
    "    b = labels_test[i]\n",
    "    if np.array_equal(a, b):\n",
    "        success += 1\n",
    "\n",
    "print(\"Total = %d Success = %d Accuracy = %f\" %\n",
    "      (len(features_test), success, success * 100 / len(features_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "06557ee6a5680432bfd1ed40c382608e35d87a61d5dba3f38c53e815f214c4c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
