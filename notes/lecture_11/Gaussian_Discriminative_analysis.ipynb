{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wzxx308Vvqrn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.stats import multivariate_normal\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTcc2qlBvqrq"
   },
   "source": [
    "# Gaussian Discriminative Analysis\n",
    "### Generative Learning Algorithm vs Discriminative Learning Algorithm\n",
    "Algorithms that try to learn mappings from input space $ \\mathcal{x} $ to the output labels $ \\mathcal{y} $ ( such as logistic regression, linear regression, etc.) are called **Discriminative Learning Algorithm(DLA).** In other words, the discriminative learning algorithm tries to learn $ y $ given $ x $. Mathematically $ P(y | x) $. On the other hand, algorithm that tries to learn $ x $ given $ y $ $ (i. e. p(x | y)) $ is called **Generative Learning Algorithm (GLA)**. Naive Bayes, Gaussian discriminant analysis are the example of GLA. While DLA tries to find a decision boundary based on the input data, GLA tries to fit a gaussian in each output label.\n",
    "![DLA vs GLA](https://github.com/gmortuza/machine-learning-scratch/blob/master/machine_learning/bayesian/gaussian_discriminative_analysis/img/GLA_DLA.png?raw=1)\n",
    "\n",
    "### Multivariate Gaussian Distribution\n",
    "Gaussian Discriminant Analysis model assumes that $ p(x | y) $ is distributed according to a multivariate normal distribution, which is parameterized by a **mean vector** $ \\mu \\in \\mathbb{R}^n $ and a **covariance matrix** $ \\Sigma \\in \\mathbb{R}^{nxn} $. This can also be written as $ \\mathcal{N}(\\mu, \\Sigma) $ and it's density function can be given by:\n",
    "$$ \n",
    "p(x;\\mu,\\Sigma) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp\\Bigg(-\\frac{1}{2}\\big(x - \\mu\\big)^T\\Sigma^{-1}\\big(x - \\mu\\big)\\Bigg)\n",
    "$$\n",
    "The mean vector and covariance matrix will determine the shape of the probability density function. In the following section you can play with these parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "674cb49b859c4df4862f0d3f6ac2a6f6"
     ]
    },
    "id": "yOVAjDs-vqrr",
    "outputId": "3ecb77fb-f0b5-49a3-aa75-efc6badb3f02"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5de5327554440cc89d630323e749572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='mu1', max=3.0, min=-3.0), FloatSlider(value=0.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(mu1=(-3,3,0.1),  mu2=(-3,3.0,0.1), diagonal_1=(0,3.0,0.1), diagonal_2=(0,3.0,0.1), non_diagonal=(-3,3.0,0.1))\n",
    "def visualize_multivariate_gaussian(mu1=0.0, mu2=0.0, diagonal_1=1, diagonal_2=1, non_diagonal=0):\n",
    "    # This code snippet is taken from here [https://scipython.com/blog/visualizing-the-bivariate-gaussian-distribution/]\n",
    "    N = 300\n",
    "    X = np.linspace(-3, 3, N)\n",
    "    Y = np.linspace(-3, 4, N)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "    # Mean vector and covariance matrix\n",
    "    mu = np.array([mu1, mu2])\n",
    "    Sigma = np.array([[ diagonal_1 , non_diagonal], [non_diagonal,  diagonal_2]])\n",
    "    print(\"𝜇 = \", mu)\n",
    "    print(\"Σ = \", Sigma)\n",
    "    # Pack X and Y into a single 3-dimensional array\n",
    "    pos = np.empty(X.shape + (2,))\n",
    "    pos[:, :, 0] = X\n",
    "    pos[:, :, 1] = Y\n",
    "\n",
    "    def multivariate_gaussian(pos, mu, Sigma):\n",
    "        \"\"\"Return the multivariate Gaussian distribution on array pos.\n",
    "\n",
    "        pos is an array constructed by packing the meshed arrays of variables\n",
    "        x_1, x_2, x_3, ..., x_k into its _last_ dimension.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n = mu.shape[0]\n",
    "        Sigma_det = np.linalg.det(Sigma)\n",
    "        Sigma_inv = np.linalg.inv(Sigma)\n",
    "        N = np.sqrt((2*np.pi)**n * Sigma_det)\n",
    "        # This einsum call calculates (x-mu)T.Sigma-1.(x-mu) in a vectorized\n",
    "        # way across all the input variables.\n",
    "        fac = np.einsum('...k,kl,...l->...', pos-mu, Sigma_inv, pos-mu)\n",
    "\n",
    "        return np.exp(-fac / 2) / N\n",
    "\n",
    "    # The distribution on the variables X, Y packed into pos.\n",
    "    Z = multivariate_gaussian(pos, mu, Sigma)\n",
    "\n",
    "    # Create a surface plot and projected filled contour plot under it.\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n",
    "                    cmap=cm.viridis)\n",
    "\n",
    "    cset = ax.contourf(X, Y, Z, zdir='z', offset=-0.15, cmap=cm.viridis)\n",
    "\n",
    "    # Adjust the limits, ticks and view angle\n",
    "    ax.set_zlim(-0.15,0.2)\n",
    "    ax.set_zticks(np.linspace(0,0.2,5))\n",
    "    ax.view_init(27, -21)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8ajx6YYvqru"
   },
   "source": [
    "![Multivariate gaussian](https://github.com/gmortuza/machine-learning-scratch/blob/master/machine_learning/bayesian/gaussian_discriminative_analysis/img/multivariate_gaussian.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4-fvzS3vqrv"
   },
   "source": [
    "### Gaussian Discriminant Analysis(GDA) model\n",
    "GDA is perfect for the case where the problem is a classificaiton problem and input variable is continious and fall into a gaussian distribution. Now lets make a flower classifier model using the iris dataset. We will apply GDA model which will model $ p(x|y) $ using a multivariate normal distribution. Iris dataset have 3 labels/class. Those are setosa, versicolor and virginica. For mathemetical modelling we will denote setosa as class 0, versicolor as class 1, virginica as class 2.\n",
    "During the training process at first we will calculate the **class probability** for each class. Class probability indicates how often data is present in the training set. For example, if we have 100 training example and class 'versicolor' appears 13 times then the class probability for class 'versicolor' will be $ \\phi_1 = \\frac{13}{100} = 0.13 $. So generally,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_k &= \\frac{\\text{number of occurance of class k}}{\\text{total number of training example}} \\\\\n",
    "        &= \\frac{1}{m}\\sum_{i=1}^m1\\{y^{(i)} = k \\}\n",
    "\\end{align}\n",
    "$$\n",
    "Here *m* is the number of training example, *k* is a specific class and $ \\phi_k $ represents the class probability of class *k*.\n",
    "\n",
    "Now we will fit a gaussian in each of the classes. For example, if we fit a gaussian for all the training data for class 0 we get it's density function as:\n",
    "$$\n",
    "p(x | y=0) \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)\n",
    "$$\n",
    "Here $ \\mu_0 $ is the mean and $ \\Sigma_0) $ is the covariance matrix for all the training set of class 0. More generally for class *k* we get the density function:\n",
    "$$\n",
    "p(x | y=k) \\sim \\mathcal{N}(\\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_k|^{1/2}}exp\\Bigg(-\\frac{1}{2}\\big(x - \\mu_k\\big)^T\\Sigma_k^{-1}\\big(x - \\mu_k\\big)\\Bigg)\n",
    "$$\n",
    "\n",
    "This density function is parameterized by mean($ \\mu $) and covariance matrix($ \\Sigma $). So, we need to find mean and covariance matrix for all classes. We can find mean $ \\mu $ and covariance matrix, $ \\Sigma $ for class *k* using the following equation.\n",
    "$$ \n",
    "\\mu_k = \\frac{\\sum_{i=1}^m 1 \\{y^{(i)} = k\\} x^{(i)}}{\\sum_{i=1}^m 1 \\{y^{(i)} = k\\}} \\\\\n",
    "\\Sigma = \\frac{1}{m}\\sum_{i=1}^m \\big(x^{(i)} - \\mu_{y^{(i)}}\\big) \\big(x^{(i)} - \\mu_{y^{(i)}}\\big)^T\n",
    "$$\n",
    "\n",
    "Before going into the prediction let's implement the theory that we discussed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wmwuwj5vvqrv"
   },
   "outputs": [],
   "source": [
    "def fit(x_train, y_train):\n",
    "    m = y_train.shape[0] # Number of training example\n",
    "    #Reshapeing the training set\n",
    "    x_train = x_train.reshape(m, -1)\n",
    "    input_feature = x_train.shape[1] # Number of input feature. In our case it's 4\n",
    "    class_label = len(np.unique(y_train.reshape(-1))) # Number of class. In our case its 3.\n",
    "    \n",
    "    # Start everything with zero first.\n",
    "    # Mean for each class. Each row contains an individual class. And each of the class input is 4 dimenstional\n",
    "    mu = np.zeros((class_label, input_feature))\n",
    "    # Each row will conatain the covariance matrix of each class.\n",
    "    # The covariance matrix is a square symettric matrix.\n",
    "    # It indicates how each of the input feature varies with each other.\n",
    "    sigma = np.zeros((class_label, input_feature, input_feature))\n",
    "    # Prior probability of each class.\n",
    "    # Its the measure of knowing the likelihood of any class before seeing the input data.\n",
    "    phi = np.zeros(class_label)\n",
    "\n",
    "    for label in range(class_label):\n",
    "        # Seperate all the training data for a single class\n",
    "        indices = (y_train == label)\n",
    "        \n",
    "        phi[label] = float(np.sum(indices)) / m\n",
    "        mu[label] = np.mean(x_train[indices, :], axis=0)\n",
    "        # Instead of writting the equation we used numpy covariance function. \n",
    "        sigma[label] = np.cov(x_train[indices, :], rowvar=0)\n",
    "    \n",
    "    return phi, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n2TnV5bvqrx"
   },
   "source": [
    "### Making prediction:\n",
    "\n",
    "Now let's assume we have some data *'x_test'* that we want to predict. First we will match our data with each of the class. We will use the gaussian distribution($ \\mu, \\Sigma $) of the classes that we calculated during the training process. This will give us the probability of *'x_test'* being in a specific class. For example, the following equation will give us the probability of *x_text* being the class 0.\n",
    "\n",
    "$$\n",
    "p(y=0 | x\\_test) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_0|^{1/2}}exp\\Bigg(-\\frac{1}{2}\\big(x\\_test - \\mu_0\\big)^T\\Sigma_0^{-1}\\big(x\\_test - \\mu_0\\big)\\Bigg)\n",
    "$$\n",
    "\n",
    "We will calculate $ p(y=k|x\\_test) $ for each class k, where $ k \\in \\{0, 1, 2\\} $. During the training we also calculated the class probability($ \\phi $) of each of the class. We will multiply class probability with the gaussian probability to get the overall probability of a class. \n",
    "\n",
    "$$ p(y=k) \\rightarrow p(y=k| x\\_test) \\times \\phi_k $$\n",
    "\n",
    "For whichever classes the probability value is higest we will consider that as the *x_test's* class. So we will maximize $ p(y=k) $. Probability can be a small number. So its better to maximum the log-likelihood of the data. \n",
    "$$ log\\big(p(y=k)\\big) = \\big(p(y=k| x\\_test)\\big) + log\\big(\\phi_k\\big) $$.\n",
    "Lets say, for different classes we got the following probability:\n",
    "\n",
    "\n",
    "<div style=\"font-size:20px;margin-top:15px;\">\n",
    "    <ul>\n",
    "        <li>setosa(0) $ \\rightarrow log\\big(\\phi_0\\big) \\times  log\\big(p(y=0 | x\\_test\\big) = -1.05 + (-169.74) = -170.80 $</li>\n",
    "<li>versicolor(1) $ \\rightarrow log\\big(\\phi_1\\big) \\times  log\\big(p(y=1 | x\\_test\\big) = -1.10 + (-1.05) = -2.16 $</li>\n",
    "<li>virginica(2) $ \\rightarrow log\\big(\\phi_2\\big) \\times  log\\big(p(y=2 | x\\_test\\big) = -1.13 + (-10.15) = -11.28 $</li>\n",
    "        </ul>\n",
    "</div>\n",
    "\n",
    "Here, class versicolor(1) got the highest value. So the prediction of *x_test* will be versicolor. Now lets implement our prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7uhM9XLvqry"
   },
   "outputs": [],
   "source": [
    "def predict(x_tests, phi, mu, sigma):\n",
    "    # flatten the training data\n",
    "    x_tests = x_tests.reshape(x_tests.shape[0], -1)\n",
    "    class_label = mu.shape[0] # Number of label we have in our case it's k = 3\n",
    "    scores = np.zeros((x_tests.shape[0], class_label))  # Initially we set the each class probability to zero.\n",
    "    for label in range(class_label): # We will calculate the probability for each of the class.\n",
    "        # normal_distribution_prob.logpdf Will give us the log value of the PDF that we just mentioned above.\n",
    "        normal_distribution_prob = multivariate_normal(mean=mu[label], cov=sigma[label])\n",
    "        # x_test can have multiple test data we will calculate the probability of each of the test data\n",
    "        for i, x_test in enumerate(x_tests):\n",
    "            scores[i, label] = np.log(phi[label]) + normal_distribution_prob.logpdf(x_test)\n",
    "    predictions = np.argmax(scores, axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlVAIlD2vqr0"
   },
   "source": [
    "### Test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMfCfW23vqr1",
    "outputId": "7721c7a2-f800-4370-d9f2-0b76f1b2dab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of our model:  1.0\n",
      "f1 score of scikit-learn model is:  1.0\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "phi, mu, sigma = fit(x_train, y_train)\n",
    "y_predict = predict(x_test, phi, mu, sigma)\n",
    "score = f1_score(y_test, y_predict, average=\"weighted\")\n",
    "print(\"f1 score of our model: \", score)\n",
    "\n",
    "# Compare this model with scikitlearn LinearDiscriminatorAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(x_train, y_train)\n",
    "y_predict_sk = lda.predict(x_test)\n",
    "print(\"f1 score of scikit-learn model is: \", f1_score(y_test, y_predict_sk, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHmj0eqbvqr4"
   },
   "source": [
    "### Conclusion\n",
    "One of biggest advantages of GLA model is it doesn't have any hyperparameter. This model works really well if the input dataset follows the gaussian distribution. If all the class share the same covariance matrix then the model is called Linear Discriminant Analysis(LDA) and if each class have different covariance matrix then the model called Quadratic Discriminant Analysis(QDA). \n",
    "Both the Logistic regression and GDA are classification algorithm and they share an interesting relationship. If we view the quantity of $ p(y=1 |x; \\phi_k, \\mu_k, \\Sigma_k) $ as a function of x we will get the logistic/sigmoid function. So, when would we prefer one model over another? \n",
    "\n",
    "**GDA**\n",
    "- makes stronger modeling assumptions \n",
    "- requies less data\n",
    "- works great if the input data follows gaussian distribution.\n",
    "- Have no hyperparameter\n",
    "\n",
    "**Logistic regression**\n",
    "- makes weaker assumption about the data.\n",
    "- requires larger dataset\n",
    "- works better than GDA if the input data are non-gaussian thats why this algorithm used often than GDA\n",
    "- Have learning rate as a hyperparameter\n",
    "\n",
    "#### GDA as dimensionality reduction\n",
    "\n",
    "Like PCA(Principal Component Analysis) this model can be used to reduce the dimensionality of the data as well. PCA finds the axes with maximum variance for the whole data set where LDA tries to find the axes for best class seperability. In practice, often a LDA is done followed by a PCA for dimensionality reduction.\n",
    "![LDA vs PCA](https://github.com/gmortuza/machine-learning-scratch/blob/master/machine_learning/bayesian/gaussian_discriminative_analysis/img/LDA_PCA.png?raw=1)\n",
    "taken from: https://sebastianraschka.com/Articles/2014_python_lda.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBUH7UdOvqr4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Gaussian Discriminative analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
