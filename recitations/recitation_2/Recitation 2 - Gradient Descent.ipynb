{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ccf834",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # To work with arrays\n",
    "import time\n",
    "import matplotlib.pyplot as plt # for showing plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230c99d",
   "metadata": {},
   "source": [
    "# Generate a simple line without intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece0f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0,100, 1000)                 #### Generate 1000 random points in range (0,100) uniformly\n",
    "B = 2.1438                                         #### Arbitrary TRUE slope \n",
    "y = B * x                                          #### Calculate y values \n",
    "plt.plot(x,y, c='blue', label='best line')         #### Plot y(x)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "y_gt = y + np.random.normal(0, 100, len(x))        #### Add Gaussian Noise to the generate a real dataset (samples)\n",
    "plt.plot(x, y_gt ,'.', c='red', label='Noisy data') ### Plot red points of the samples \n",
    "plt.plot(x, -5*x, c='green', label='Initial guess') ### Plot a line of initial guess y = (-5) * x\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1914a",
   "metadata": {},
   "source": [
    "# MSE Loss and its gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092011e",
   "metadata": {},
   "source": [
    "$$ loss = MSE = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - Bx_i)^2$$\n",
    "$$ \\frac{\\partial L}{\\partial B} = \\frac{1}{n} \\sum_{i=1}^{n} (-x_i)(y_i-Bx_i)$$\n",
    "$$ \\frac{\\partial L}{\\partial B} = \\frac{1}{n} \\sum_{i=1}^{n} x_i(Bx_i-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faefc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x , B):\n",
    "    return np.multiply(B , x)\n",
    "\n",
    "def MSE(y , pred):\n",
    "    return 0.5 * np.mean(np.power(y- pred , 2)) \n",
    "\n",
    "def gradient(x, y, B):\n",
    "    return np.mean(np.multiply(x , (B*x-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358bed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data , init_B, num_epochs , lr):\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    B = init_B\n",
    "    B_list = []                    ## History of B\n",
    "    loss_list = []                 ## History of Loss values\n",
    "    for i in range(num_epochs):\n",
    "        B_list.append(B) ## Adds the current B to the list\n",
    "        ## 1) Compute Loss\n",
    "        pred = predict(x, B)\n",
    "        loss = MSE(y, pred) \n",
    "        loss_list.append(loss)\n",
    "        ## 2) Compute Gradient\n",
    "        grad = gradient(x, y, B)\n",
    "        ## 3) Update the Model's Parameter\n",
    "        B = B - lr * grad\n",
    "        \n",
    "    B_list.append(B)        ## Save the final Coefficient\n",
    "    pred = predict(x, B) \n",
    "    loss = MSE(y , pred)\n",
    "    loss_list.append(loss)  ## Save the final loss value\n",
    "    \n",
    "    return B_list , loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA SHAPE \n",
    "print(\"Shape of x is: \", x.shape)\n",
    "print(\"Shape of y is: \", y_gt.shape)\n",
    "data = np.vstack((x ,y_gt))\n",
    "print(\"Shape of data is: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdbba5",
   "metadata": {},
   "source": [
    "### Solve Linear Regression with Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, loss = gradient_descent(data , init_B=-5 , num_epochs=20, lr=0.0005) \n",
    "print(\"final B is: \", B[-1])\n",
    "print(\"final Loss is: \", loss[-1])\n",
    "\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.xlabel(\"Epoch = Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Iteration\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(B)\n",
    "plt.xlabel(\"Epoch = Iteration\")\n",
    "plt.ylabel(\"B\")\n",
    "plt.title(\"B vs. Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab03f7",
   "metadata": {},
   "source": [
    "## Effect of noise magnitude (data distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt = y + np.random.normal(0, 100, len(x))\n",
    "plt.plot(x, y_gt,'.',c='red', label=r'$\\sigma = 100$')\n",
    "\n",
    "y_gt = y + np.random.normal(0, 10, len(x))\n",
    "plt.plot(x, y_gt,'.',c='orange', label=r'$\\sigma = 10$')\n",
    "plt.plot(x,y ,c='blue', label='True Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f629428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True B coefficient is : \", 2.1438)\n",
    "for noise in [0.001 , 1 , 10, 100, 1000, 10000]:\n",
    "    y_gt = y + np.random.normal(0, noise, len(x))\n",
    "    data = np.vstack((x ,y_gt))\n",
    "    B, loss = gradient_descent(data , -5 , 50, 0.0001)\n",
    "    print(f\"final B for noise {noise} is: \", round(B[-1],5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6b1b7",
   "metadata": {},
   "source": [
    "## Effect of Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt = y + np.random.normal(0, 10, len(x))\n",
    "data = np.vstack((x ,y_gt))\n",
    "    \n",
    "for lr in [1e-6,0.00001 , 0.0001 , 0.0005, 0.001]:\n",
    "    B, loss = gradient_descent(data , -5 , 100, lr )\n",
    "    print(f\"final B for learning rate {lr} is: \", round(B[-1],5))\n",
    "    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    ax1 = plt.subplot(1,2, 1)\n",
    "    ax1.plot(loss)\n",
    "    ax1.set_xlabel(\"epoch\")\n",
    "    ax1.set_ylabel(\"loss\")\n",
    "    plt.title(f\"LR is {lr}\")\n",
    "    \n",
    "    ax2 = plt.subplot(1,2, 2)\n",
    "    ax2.plot(B)\n",
    "    ax2.set_xlabel(\"epoch\")\n",
    "    ax2.set_ylabel(\"B\")\n",
    "    \n",
    "    plt.title(f\"LR is {lr}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272ef57",
   "metadata": {},
   "source": [
    "## Loss Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ba379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_landscape():\n",
    "    y_gt = y + np.random.normal(0, 10, len(x))\n",
    "    B_arr = np.linspace(-6, 10 , 100)\n",
    "    losses = []\n",
    "    for B in B_arr:\n",
    "        pred = predict(x, B)\n",
    "        loss = MSE(y_gt, pred)\n",
    "        losses.append(loss)\n",
    "\n",
    "    plt.plot(B_arr, losses)\n",
    "    plt.title(\"Loss landscape\")\n",
    "    plt.xlabel(\"B\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    \n",
    "plot_landscape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ea9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(data, lr, num_epoch, init_B):\n",
    "    B, loss = gradient_descent(data , init_B , num_epoch, lr)\n",
    "    print(\"final B is: \", B[-1])\n",
    "    plot_landscape()\n",
    "    for i in range(len(B)-1):\n",
    "        plt.scatter(B[i], loss[i], c='red')\n",
    "        plt.plot([B[i], B[i+1]], [loss[i], loss[i+1]], linestyle='dashed', c='grey')\n",
    "    plt.show()\n",
    "    \n",
    "data = np.vstack((x ,y_gt))\n",
    "\n",
    "plot_traj(data, 0.00001, 10, -5)\n",
    "\n",
    "plot_traj(data, 0.0001, 10, -5)\n",
    "\n",
    "plot_traj(data, 0.0005, 10, -5)\n",
    "\n",
    "plot_traj(data, 0.00063, 3, -5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa74b11",
   "metadata": {},
   "source": [
    "## Momentum Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_gd(data , init_B, num_epochs , lr):\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    B = init_B\n",
    "    B_list = []\n",
    "    loss_list = []\n",
    "    v = 0\n",
    "    beta = 0.9\n",
    "    for i in range(num_epochs):\n",
    "        B_list.append(B) ## Adds the current B to the list\n",
    "        ## 1) Compute loss\n",
    "        pred = predict(x, B)\n",
    "        loss = MSE(y, pred)\n",
    "        loss_list.append(loss)\n",
    "        ## 2) Compute Gradient\n",
    "        grad = gradient(x, y, B)\n",
    "        ## 3) Update with Momentum\n",
    "        v = beta*v + (1-beta)*(grad)\n",
    "        v_corr = v / (1-beta**(i+1) )\n",
    "        B = B - lr * v_corr\n",
    "        \n",
    "    B_list.append(B)\n",
    "    pred = predict(x, B)\n",
    "    loss = MSE(y, pred)\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    return B_list , loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(data, lr, num_epoch, init_B):\n",
    "    B, loss = momentum_gd(data , init_B , num_epoch, lr)\n",
    "    print(\"final B is: \", B[-1])\n",
    "    plot_landscape()\n",
    "    for i in range(len(B)-1):\n",
    "        plt.scatter(B[i], loss[i], c='red')\n",
    "        plt.plot([B[i], B[i+1]], [loss[i], loss[i+1]], linestyle='dashed', c='grey')\n",
    "    plt.show()\n",
    "    \n",
    "data = np.vstack((x ,y_gt))\n",
    "\n",
    "plot_traj(data, 0.00001, 10, -5)\n",
    "\n",
    "plot_traj(data, 0.0001, 100, -5)\n",
    "\n",
    "plot_traj(data, 0.0005, 10, -5)\n",
    "\n",
    "plot_traj(data, 0.00063, 100, -5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477f573",
   "metadata": {},
   "source": [
    "# Batch, SGD, Mini-Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d0d24",
   "metadata": {},
   "source": [
    "### Iteration vs. Epoch\n",
    "Iteration is about updating the model's weights and coefficients (Parameters). Each time we update the parameters, we call it an iteration. \n",
    "Epoch is about the dataset. Each time we have used all the samples in the dataset to update our model, we call it an epoch. \n",
    "Therefore, if we use full batch gradient descent, an iteration is equivalent to an epoch. \n",
    "On the other hand, for Stochastic GD, we update the model after visiting each sample. So, after N iterations where N is the dataset size, we have performed an epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441650a3",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data , init_B, num_epochs , lr):\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    B = init_B\n",
    "    B_list = []\n",
    "    loss_list = []\n",
    "    for i in range(num_epochs):\n",
    "        B_list.append(B) ## Adds the current B to the list\n",
    "        pred = predict(x , B)\n",
    "        loss = MSE(y , pred)\n",
    "        loss_list.append(loss)\n",
    "        grad = gradient(x , y , B)\n",
    "        B = B - lr * grad\n",
    "        \n",
    "    B_list.append(B)\n",
    "    pred = predict(x , B)\n",
    "    loss = MSE(y , pred)\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    return B_list , loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0,100, 1000)\n",
    "B = 2.1438\n",
    "y = B * x\n",
    "y_gt = y + np.random.normal(0, 100, len(x))\n",
    "data = np.vstack((x ,y_gt))\n",
    "B, loss = gradient_descent(data , -5 , 10, 0.0001)\n",
    "print(\"Final B after 10 epochs is: \", B[-1])\n",
    "plt.plot(loss)\n",
    "plt.xlabel(\"Iteration = Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee278c",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(data , init_B, num_iter , lr):\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    B = init_B\n",
    "    B_list = []\n",
    "    loss_list = []\n",
    "    for i in range(num_iter):\n",
    "        j = i % len(x)\n",
    "        B_list.append(B) ## Adds the current B to the list\n",
    "        loss = 0.5 * np.power(y[j]- B*x[j] , 2) \n",
    "        loss_list.append(loss)\n",
    "        grad = np.multiply(x[j] , (B*x[j]-y[j])) \n",
    "        B = B - lr * grad\n",
    "        \n",
    "    B_list.append(B)\n",
    "    loss = 0.5 * np.mean(np.power(y- B*x , 2))\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    return B_list , loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, loss = SGD(data , -5 , 10000, 0.000001)\n",
    "print(\"Final B after 10000 iterations = 10 epoch is: \", B[-1])\n",
    "plt.plot(loss)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce9bf9",
   "metadata": {},
   "source": [
    "## Mini-Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b416b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mini_GD(data , init_B, num_iter , lr, batch_size):\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    B = init_B\n",
    "    B_list = []\n",
    "    loss_list = []\n",
    "    num_batches = len(x) // batch_size\n",
    "    num_epochs = num_iter // num_batches \n",
    "    for epoch in range(num_epochs):\n",
    "        for b in range(num_batches):\n",
    "            B_list.append(B) ## Adds the current B to the list\n",
    "            loss = 0.5 * np.mean(np.power(y[b*batch_size: (b+1)*batch_size]- B*x[b*batch_size: (b+1)*batch_size] , 2)) \n",
    "            loss_list.append(loss)\n",
    "            grad = np.mean(np.multiply(x[b*batch_size: (b+1)*batch_size] , (B*x[b*batch_size: (b+1)*batch_size]-y[b*batch_size: (b+1)*batch_size]))) \n",
    "            B = B - lr * grad\n",
    "        \n",
    "    B_list.append(B)\n",
    "    loss = 0.5 * np.mean(np.power(y- B*x , 2))\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    return B_list , loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd746ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, loss = Mini_GD(data , -5 , 100, 0.0001, 100)\n",
    "print(\"Final B after 100 iterations = 10 epoch is: \", B[-1])\n",
    "plt.plot(loss)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fede84",
   "metadata": {},
   "source": [
    "## Loss Landscape for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aaebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt = y + np.random.normal(0, 30, len(x))\n",
    "B_arr = np.linspace(1, 4 , 100)\n",
    "\n",
    "loss_arr = np.zeros((4, len(B_arr)))\n",
    "for t in range(len(B_arr)):\n",
    "    loss_arr[0][t]  = 0.5 * np.power(y_gt[0]-B_arr[t]*x[0] , 2)\n",
    "    loss_arr[1][t]  = 0.5 * np.power(y_gt[1]-B_arr[t]*x[1] , 2)\n",
    "    loss_arr[2][t]  = 0.5 * np.power(y_gt[2]-B_arr[t]*x[2] , 2)\n",
    "    \n",
    "    loss_arr[3][t] = 0.5 * np.mean(np.power(y_gt-B_arr[t]*x , 2))\n",
    "    \n",
    "    \n",
    "    \n",
    "plt.plot(B_arr, loss_arr[0], label='Sample 1')\n",
    "plt.plot(B_arr, loss_arr[1], label='Sample 2')\n",
    "plt.plot(B_arr, loss_arr[2], label='Sample 3')\n",
    "plt.plot(B_arr, loss_arr[3], label='All Samples')\n",
    "plt.legend()\n",
    "plt.title(\"Loss landscape\")\n",
    "plt.xlabel(\"B\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8512ebf",
   "metadata": {},
   "source": [
    "## True or False\n",
    "### Using SGD we don't have the real loss landscape, but we estimate it.  (T or F) TRUE\n",
    "### But using Batch GD, we have the real loss landscape, and have the exact gradient. (T or F) FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5987e2",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d944d",
   "metadata": {},
   "source": [
    "A nice visualization of GD, SGD and Mini-Batch: http://www.deeplearning.ai/ai-notes/optimization/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "06557ee6a5680432bfd1ed40c382608e35d87a61d5dba3f38c53e815f214c4c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
